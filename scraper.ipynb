{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:For April2021: did not scrape correctly\n",
      "CRITICAL:root:For May2021: did not scrape correctly\n",
      "CRITICAL:root:For June2021: did not scrape correctly\n",
      "CRITICAL:root:For July2021: did not scrape correctly\n",
      "CRITICAL:root:For August2021: did not scrape correctly\n",
      "CRITICAL:root:For September2021: did not scrape correctly\n",
      "CRITICAL:root:For October2021: did not scrape correctly\n",
      "CRITICAL:root:For November2021: did not scrape correctly\n",
      "CRITICAL:root:For December2021: did not scrape correctly\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('January', 2021) 4,172\n",
      "('January', 2020) 311m\n",
      "('January', 2019) 451m\n",
      "('February', 2021) 3,998\n",
      "('February', 2020) 307m\n",
      "('February', 2019) 477m\n",
      "('March', 2021) 4,628\n",
      "('March', 2020) 739m\n",
      "('March', 2019) 550m\n",
      "('April', 2021) -1\n",
      "('April', 2020) 662\n",
      "('April', 2019) 428\n",
      "('May', 2021) -1\n",
      "('May', 2020) 438\n",
      "('May', 2019) 262\n",
      "('June', 2021) -1\n",
      "('June', 2020) 570\n",
      "('June', 2019) 382\n",
      "('July', 2021) -1\n",
      "('July', 2020) 3,251\n",
      "('July', 2019) 2,426\n",
      "('August', 2021) -1\n",
      "('August', 2020) 2,935\n",
      "('August', 2019) 2,147\n",
      "('September', 2021) -1\n",
      "('September', 2020) 3,841\n",
      "('September', 2019) 3,541\n",
      "('October', 2021) -1\n",
      "('October', 2020) 3,429\n",
      "('October', 2019) 3,054\n",
      "('November', 2021) -1\n",
      "('November', 2020) 5,244\n",
      "('November', 2019) 3,974\n",
      "('December', 2021) -1\n",
      "('December', 2020) 5,806\n",
      "('December', 2019) 4,732\n"
     ]
    }
   ],
   "source": [
    "from requests import get\n",
    "from requests.exceptions import RequestException\n",
    "from contextlib import closing\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import logging\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections import deque\n",
    "\n",
    "# month in 1st %s, year in 2nd %s\n",
    "URL = 'https://venturebeat.com/%s-%s-npd'\n",
    "\n",
    "\n",
    "class Scraper:\n",
    "    def __init__(self, request_delay):\n",
    "        \"\"\"\n",
    "        request_delay - delay in seconds before another request can be made\n",
    "        if a 429 Too many requests error is encountered\n",
    "        \"\"\"\n",
    "        self.cache = deque()\n",
    "        self.request_delay = request_delay\n",
    "\n",
    "    def simple_get(self, url):\n",
    "        \"\"\"\n",
    "        Attempts to get the content at `url` by making an HTTP GET request.\n",
    "        If the content-type of response is some kind of HTML/XML, return the\n",
    "        text content, otherwise return None.\n",
    "        \"\"\"\n",
    "        logging.info('Scraper: Requested \"' + url + '\"')\n",
    "\n",
    "        try:\n",
    "            while True:\n",
    "                with closing(get(url, stream=True)) as resp:\n",
    "                    # Encountered 429; sleep and try again\n",
    "                    if resp.content == b'Too Many Requests\\n':\n",
    "                        logging.info('429 Too Many Requests; sleeping for ' + str(self.request_delay) + ' seconds...')\n",
    "                        time.sleep(self.request_delay)\n",
    "                        continue\n",
    "\n",
    "                    return resp.content\n",
    "        except RequestException as e:\n",
    "            logging.critical('Scraper: Error during requests to {0} : {1}'.format(url, str(e)))\n",
    "            raise e\n",
    "\n",
    "    def get_software_usd(self, month, year):\n",
    "        \"\"\"\n",
    "        month - string\n",
    "        year - string\n",
    "        \n",
    "        Returns a tuple: (amount for year-1, amount for year)\n",
    "        where amount is in USD, millions, as a string\n",
    "        \"\"\"\n",
    "        try:\n",
    "            raw_html = self.simple_get(URL % (month, year))\n",
    "            html = BeautifulSoup(raw_html, 'html.parser')\n",
    "\n",
    "            all_td = html.find_all('td')\n",
    "\n",
    "            start = -1            \n",
    "            for i, s in enumerate(all_td):\n",
    "                s_str = str(s)\n",
    "                if 'Video Game Software (Physical and Full' in s_str or 'Video Game Content' in s_str or 'Software' in s_str:\n",
    "                    start = i\n",
    "                    break\n",
    "            if start == -1:\n",
    "                logging.critical('For ' + month + year + ': did not scrape correctly')\n",
    "            else:\n",
    "                if '%' in all_td[start + 2].contents[0] or '%' in all_td[start + 3].contents[0]:\n",
    "                    return (all_td[start + 1].contents[0][1:], all_td[start + 2].contents[0][1:])\n",
    "                else:\n",
    "                    # all_td[start + 3].contents[0] = \"$xx\"\n",
    "                    return (all_td[start + 2].contents[0][1:], all_td[start + 3].contents[0][1:])\n",
    "                \n",
    "            return (-1, -1)\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "scraper = Scraper(20)\n",
    "months = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
    "years = [2021, 2020]\n",
    "d = {}\n",
    "for m in months:\n",
    "    for y in years:\n",
    "        amounts = scraper.get_software_usd(m, str(y))\n",
    "        d[(m, y)] = amounts[1]\n",
    "        d[(m, y-1)] = amounts[0]\n",
    "\n",
    "csv_file = './Datasets/gamesales.csv'\n",
    "with open(csv_file, 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=['Month', 'Year', 'Amount (USD millions)'])\n",
    "    writer.writeheader()\n",
    "    for k, v in d.items():\n",
    "        print(k, v)\n",
    "        writer.writerow({'Month': k[0], 'Year': k[1], 'Amount (USD millions)': v})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
